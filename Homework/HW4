import numpy as np
import matplotlib.pyplot as plt
import pylab as pl
import pandas as pd
%matplotlib inline
from sklearn.decomposition import PCA

#bring in dataset
from sklearn.datasets import fetch_olivetti_faces
faces = fetch_olivetti_faces()

print faces

print(faces.DESCR)

X = faces.data
y = faces.target
images = faces.images

pca = PCA(n_components=2)
X_r = pca.fit(X).transform(X)
X_r

from sklearn.neighbors import KNeighborsClassifier
from sklearn.cross_validation import KFold
model = KNeighborsClassifier(n_neighbors=3).fit

# generic cross validation function
def cross_validate(X, y, classifier, k_fold) :

    # derive a set of (random) training and testing indices
    k_fold_indices = KFold( len(X), n_folds=k_fold,
                           shuffle=True,
                           random_state=0)

    k_score_total = 0
    # for each training and testing slices run the classifier, and score the results
    for train_slice, test_slice in k_fold_indices :

        model = classifier(X[ train_slice  ],
                         y[ train_slice  ])

        k_score = model.score(X[ test_slice ],
                              y[ test_slice ])

        k_score_total += k_score

    # return the average accuracy
    return k_score_total/k_fold
    
print "Results from original features (each combo of 2 features):"
print cross_validate(X[:,[0,1]], y, model, 10)
print cross_validate(X[:,[0,2]], y, model, 10)
print cross_validate(X[:,[0,3]], y, model, 10)
print cross_validate(X[:,[1,2]], y, model, 10)
print cross_validate(X[:,[1,3]], y, model, 10)
print cross_validate(X[:,[2,3]], y, model, 10)
print
print "Results from PCA (2 components):"
print cross_validate(X_r, y, model, 10)

# Percentage of variance explained (first two components):
print "First component: " + str(pca.explained_variance_ratio_[0])
print "Second component: " + str(pca.explained_variance_ratio_[1])

pl.figure()
for c, i, images in zip("rgb", [0, 1, 2], images):
    pl.scatter(X_r[y == i, 0], X_r[y == i, 1], c=c, label=images)
pl.legend()
pl.title('PCA of Faces dataset')

pl.show()

Change from PCA to RandomizedPCA. Do you obtain the same principal components? Why? Why not?
from sklearn.decomposition import RandomizedPCA

X = np.array([[0, 1], [0, 2], [0, 3], [1, 1], [1, 2], [1, 3]]) 
pca = RandomizedPCA(n_components=2)
pca.fit(X)                 
RandomizedPCA(copy=True, n_components=2,
       random_state=None, whiten=False)
print(pca.explained_variance_ratio_) 

print "First component: " + str(pca.explained_variance_ratio_[0])
print "Second component: " + str(pca.explained_variance_ratio_[1])

pl.figure()
for c, i, images in zip("rgb", [0, 1, 2], images):
    pl.scatter(X_r[y == i, 0], X_r[y == i, 1], c=c, label=images)
pl.legend()
pl.title('PCA of Faces dataset')

pl.show()

Implement k-means clustering, using sklearn, with k = number of people in the dataset. Does the clustering work? Does each cluster map correctly to one person?
from sklearn.cluster import KMeans

km = KMeans(n_clusters=3,  init='random', n_init=1 , max_iter = 1, random_state=1)

km.fit(images)
#This gives me an error that I wasn't able to fix. 

END
